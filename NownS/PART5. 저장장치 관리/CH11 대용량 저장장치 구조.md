# CH11. 대용량 저장장치 구조

# 개관

- 최신 보조저장장치는 HDD 및 비휘발성 메모리(NVM)장치가 있음

## HDD

- 원형 판 형태의 플래터 존재
- 플래터는 원형의 트랙으로 논리적으로 나누어짐
- 트랙은 다시 섹터로 나누어짐
- 동일한 암 위치의 트랙의 집합은 실린더를 형성
- 성능
    - 분당 회전수(RPM)
    - 전송 속도(데이터 흐름의 속도)
    - 위치 지정 시간/임의 액세스 시간
        - 탐색 시간 → 디스크 암을 원하는 실린더로 이동하는 시간
        - 회전 지연 시간 → 원하는 섹터가 디스크 헤드 위치까지 회전하는데 걸리는 시간

## 비휘발성 메모리 장치(NVM)

### 개요

- 디스크 드라이브와 유사한 컨테이너의 경우 → SSD
- 다른 경우 → USB 드라이브
- 수명
    - DWPD(Drive Writes Per Day) → 하루에 몇 번 쓸 수 있는지

### NAND 플래시 컨트롤러 알고리즘

- NAND는 덮어쓰기 불가, 따라서 삭제 후 새로운 내용을 작성해야 함.
- 컨트롤러에서 유효한 데이터 블록을 추적하기 위해 플래시 변환 계층(FTL)을 유지

## 휘발성 메모리

- RAM 드라이브는 보조장치처럼 작동하기도 함
- 캐시나 버퍼가 프로그래머나 운영체제에 의해 할당된다면, RAM드라이브는 사용자와 프로그래머가 임의로 데이터를 메모리에 임시 저장 가능
- 고속 임시 저장 공간으로 유용함

## 보조저장장치 연결 방법

- 시스템 버스/IO버스에 의해 컴퓨터에 연결됨
    - ATA, SATA, eSATA, SAS, USB, FC 등 여러 종류 존재
- 버스에서의 데이터 전송은 컨트롤러에 의해 수행됨
    - 버스의 컴퓨터 쪽에 있는 컨트롤러 → 호스트 컨트롤러
    - 저장장치 내장 컨트롤러 → 장치 컨트롤러

# 디스크 스케줄링

- 저장장치 IO 요청이 처리되는 순서를 관리하여 접근 시간과 대역폭을 향상시키는게 주 과제

## 선입 선처리 스케줄링

- 가장 간단한 선입선출 알고리즘

## SCAN 스케줄링

- 디스크 암이 한 끝에서 시작하여 반대 끝으로 이동
- 다른 끝에 도달하면 역방향으로 이동하면서 오는 길에 있는 요청 처리

## C-SCAN 스케줄링

- 한쪽으로 헤드를 이동하면서 요청 처리
- 한쪽 끝에 도달하면 처음 시작했던 자리로 다시 돌아가서 서비스 실행

## 디스크 스케줄링 알고리즘의 선택

- 그 외 다양한 디스크 스케줄링 알고리즘 존재
- 기아 문제를 일으킬 가능성 → 마감 시간 스케줄러를 개발

# NVM 스케줄링

- NVM의 경우 따로 헤드가 없기 때문에 디스크 스케줄링 알고리즘의 영향을 받지 않음
- 일반적으로 가장 간단한 FCFS 정책 사용
- 가비지 수집에 의해 쓰기 증폭 현상 발생 가능

# 오류 감지 및 수정

- 패리티 비트를 활용하여 오류 감지
- 네트워킹에서는 해시 함수를 활용하여 순환 중복 검사를 진행하기도 함
- 오류 수정 코드(ECC)는 문제 감지뿐 아니라 보정도 수행

# 저장장치 관리

## 드라이브 포매팅, 파티션, 볼륨

- 물리적 포매팅 → 이미 공장에서 되어서 나옴
- 이후 운영체제가 자체 데이터 구조를 장치에 기록해야 함
    1. 장치를 하나 이상의 블록, 페이지 그룹으로 파티션
    2. 볼륨 생성 및 관리
        - 대부분 암시적으로 적용
        - 명시적으로 행해지는 경우 → RAID, 파일 시스템 분산 등
    3. 논리적 포매팅/파일 시스템 생성
- 효율성을 높이기 위해 대부분의 파일 시스템은 블록을 클러스터로 묶음
    - 장치 I/O는 블록으로 수행, 파일 시스템 I/O는 클러스터로 수행
- 특정 프로그램이 파티션을 raw디스크로 사용 가능하기도 함

## 부트 블록

- 부트스트랩 로더가 부트스트랩 프로그램을 가져오기 위한 위치
- 부트 파티션이 있는 장치를 부트 디스크/시스템 디스크 라고 함

## 손상된 블록

- 컨트롤러에 의해 결함을 처리함
- 구형 컨트롤러 → 수동으로 검사를 통한 처리
- 정교한 디스크 → 손상 블록 리스트를 유지하고, 예비 섹터를 이용하여 교체 수행

# 스왑 공간 관리

- 가상 메모리를 활용하기 위한 스왑 공간 설계 및 구현 필요

## 스왑 공간 사용

- 예상보다 크게 잡는 것이 안전함 → 크래시 방지
- 스왑 공간이 너무 큰 경우 파일을 위한 공간이 부족 → 운영체제별 권장 예약 스왑 공간 크기 존재

## 스왑 공간 위치

- 일반적인 파일 시스템 루틴으로 생성할 수도 있음
- RAW파티션을 이용할 수도 있음
- 유연하게 두 가지를 모두 활용할 수 있는 경우도 있음 → 리눅스
    - 파일 시스템을 사용하면 할당과 관리가 편함
    - RAW 파티션 사용 시 성능이 좋아짐

## 스왑 공간 관리

- 현재 리눅스의 경우, 스왑 공간을 익명 메모리에만 허용
- 각 스왑 영역은 연속된 4KB의 페이지 슬롯으로 구성되고, 해당 페이지 슬롯에 스왑된 페이지 적재
- 각 스왑 영역은 스왑 맵을 통해 연관됨, 스왑 맵을 통해 할당 여부를 파악

# 저장장치 연결

## 호스트 연결 저장장치

- 로컬 I/O 포트를 통해 액세스되는 저장장치

## 네트워크 연결 저장장치

- NAS → 네트워크를 통해 저장장치에 대한 액세스 제공
- Linux → NFS, Windows → CIFS
- RFC 인터페이스를 구현하는 소프트웨어를 통해 저장장치 배열 형태로 구현됨

## 클라우드 저장장치

- 네트워크를 통해 저장장치에 액세스할 수 있음
- NAS와 차이점은 저장장치 접근 방식 및 사용자에게 제공되는 방식
    - 클라우드는 API 기반
    - WAN의 지연시간 및 장애 시나리오 때문에 API 사용

## SAN과 저장장치 배열

- SAN : 서버들과 저장장치 유닛들을 연결하는 private network(네트워크이지만 저장장치 프로토콜 활용)
    - 여러 호스트와 저장장치가 같은 SAN에 부착 가능
    - 저장장치가 동적으로 호스트에 할당 가능
    - 근거리에 있으며, 라우팅이 없으므로 더 많은 호스트 연결 가능
- 저장장치 배열 : SAN 포트, 네트워크 포트 등을 포함하는 특수목적 장치로, 컨트롤러를 통해 드라이브와 저장장치를 관리하고 저장장치에 액세스 가능

# RAID 구조

- 디스크 구성 기술을 이용하여 성능과 신뢰성 이슈를 해결하기 위한 방법

## 중복으로 신뢰성 향상

- 미러링 기술을 통하여 모든 쓰기 작업을 두 드라이브에서 동시에 실행
- 하나가 오류가 발생하면, 다른 드라이브에서 데이터를 읽어옴
- 드라이브 고장이 독립적일 수는 없지만, 그래도 높은 신뢰성 제공

## 병렬성을 통한 성능 향상

- 데이터 스트라이핑을 통해 전송 비율 향상
    - 비트 단위 스트라이핑 : 여러 드라이브에 각 바이트의 비트를 나누어 저장
        - 8개 드라이브를 쓰면 각 접근은 한번에 8배의 데이터를 읽을 수 있음
    - 블록 단위 스트라이핑 : 블록을 여러 드라이브에 거쳐 스트라이핑
- 부하 균등화를 이용하여 작은 액세스의 처리량을 높임
- 규모가 큰 액세스의 응답 시간을 줄임

## RAID 레벨

- 레벨 0
    - 블록 레벨 스트라이핑
    - 어떤 중복 정보도 갖고있지 않음
- 레벨1
    - 드라이브 미러링 사용
- 레벨 4
    - 메모리-스타일 오류 수정 코드(ECC) 구성
    - 오류 수정 계산값을 N+1 위치에 저장
- 레벨5
    - 데이터와 패리티를 모두 N+1개의 드라이브에 분산 저장
    - 패리티 드라이브에 대한 과도한 집중을 방지
- 레벨6
    - P+Q 중복 기법
    - 여러 디스크 오류에 대비하기 위해 추가 중복 정보를 저장
- 다차원 레벨 6
- 레벨0+1과 레벨 1+0
- 이처럼 다양한 레벨들이 존재하고, 다양한 선택이 가능함
- 위에서 설명하지 않은 다양한 변형 기법들이 존재

## RAID 레벨 선택

- 시스템 설계자의 선택
- 고려사항
    - 복구 능력
        - 레벨1이 가장 쉬움
    - 성능
        - 레벨0이 가장 고성능, 데이터 손실 가능성 있음
    - 각각 레벨에 따른 트레이드오프가 존재하기 때문에, 최적의 레벨을 설계자가 선택해야 함

## 확장

- 해당 RAID 개념을 이용하여 무선 데이터 브로드캐스트 등 여러 저장장치에 적용
- 여러 테이프 중 하나가 손상을 입어도 RAID 구조를 통해 데이터를 복구하기도 함
- 데이터의 일부를 수신하지 못한 경우에도 다른 유닛으로부터 복구 가능

## 문제점

- 데이터의 가용성을 항상 보장하지는 않음
- 적절히 복구되지 않아서 잘못된 데이터를 만들어 낼 수도 있음
- 물리적 오류는 보호하지만 하드웨어/소프트웨어 오류는 보호하지 못함
- ZFS에서는 이런 문제를 해결하기 위해 내부적인 체크섬을 통해 해결

## 객체 저장소

- 아마존 S3